{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec_fake_review_detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID-XFZSwWICO",
        "outputId": "9afcdd0e-5bb6-4777-f428-8bdb14c1fa03"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScEfFEMXWaG4"
      },
      "source": [
        "# Packages Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCJzesm4uXUS"
      },
      "source": [
        "pip install -q -U keras-tuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muBi9FOkWpWy"
      },
      "source": [
        "import os\n",
        "import string\n",
        "import datetime\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras_tuner as kt\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input,concatenate,Activation, Dense, Dropout, Embedding, Flatten,Bidirectional, LSTM\n",
        "from keras.models import Model\n",
        "from keras.initializers import Constant\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])\n",
        "\n",
        "# # Load the TensorBoard notebook extension\n",
        "# %load_ext tensorboard\n",
        "# # Clear any logs from previous runs on tensorboard\n",
        "# !rm -rf ./logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMOQS2zqX8n1"
      },
      "source": [
        "# Changing the parameter of the column width for the display of the pandas dataframe\n",
        "pd.set_option('display.max_colwidth',100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqwkuxPuXDkI"
      },
      "source": [
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdFmC7o6YmgG"
      },
      "source": [
        "#Dataset import and features selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isIOTwumYiML"
      },
      "source": [
        "#Check if the dataset is pre-downloaded in our working directory, otherwise we download it using the url location where is available\n",
        "if not os.path.isfile('gdrive/MyDrive/datasets/amazon_reviews.txt'):\n",
        "    url = 'https://drive.google.com/uc?id=1-LYI_s6oZ0OTe3I0vFYRYIBVhjFswReY&export=download'\n",
        "    print('Downloading DB to train')\n",
        "    wget.download(url)\n",
        "    print('Download Completed!\\nUnzipping...')\n",
        "    shutil.unpack_archive('amazon_reviews.zip')\n",
        "else:\n",
        "   print(\"The dataset is already downloaded\")   \n",
        "#The dataset is a tab seperated csv file therefore we define delimiter as delimiter = '/t'.\n",
        "#We set index_col = False since we want to discard the entries with missing columns \n",
        "df = pd.read_csv('gdrive/MyDrive/datasets/amazon_reviews.txt', error_bad_lines=False, delimiter=\"\\t\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCa8xKdKbgIb"
      },
      "source": [
        "We do feature extraction by keeping those features that will help our model perform better. For that purpose we decided to keep only the columns that are listed below : \n",
        " - **REVIEW_TEXT**: The comment description that is given as review for a product \n",
        " - **RATING**: The actual rating integer number from 1 to 5 with 1 be the lowest and 5 the highest. \n",
        " -**VERIFIED_PURCHASE**: A parameter which indicates of the user who comments is a verified on (with \"Y\") or is not (with \"N\"). Later on we are going to change those value with 1 and 0 respectively. \n",
        " -**LABEL**: This is the parameter which indicated if the relation between the \"RATING\" and the \"REVIEW_TEXT\" has a mismatch, giving the value 1 if that happen, meaning that it is a fake one, or not giving the value 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UILbOuqXXs6n"
      },
      "source": [
        "features = [\"REVIEW_TEXT\", \"RATING\", \"LABEL\", \"VERIFIED_PURCHASE\"]\n",
        "df = df[features]\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3IhcTURdeIx"
      },
      "source": [
        "# Data pre-processing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7mJTlG5dn4U"
      },
      "source": [
        "In that step with pre-process our data in order to bring them in a format that is more suitable for manipulation and it would be easier halded by our model.\n",
        "\n",
        " - Firstly,  we change our categorical feature \"VERIFIED_PURCHASE\" from \"Y\" and \"N\" to \"1\" and \"0\". This helps our model to work in a more efficient way and to \"understand\" better the \"meaning\" of this feature. \n",
        " - Secondly, we pre-processed the \"REVIE_TEXT\" column by removing all the punction marks and to make all the word lower-case. By removing the punctuation we are able to avoid adding to our vocabulary words with excactly the same meaning. This will give more capacity to our embedding vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtXiP2fldURA"
      },
      "source": [
        "# Changing the categorical values from \"Y\" and \"N\" to 1 and 0\n",
        "df['VERIFIED_PURCHASE'] = pd.Categorical(df['VERIFIED_PURCHASE'])\n",
        "df['VERIFIED_PURCHASE'] = df['VERIFIED_PURCHASE'].cat.codes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JhjkPLMfiIi"
      },
      "source": [
        "#We define a function that is going to make our word lower-case and then it will remove punctuations\n",
        "def remove_punctuation(txt):\n",
        "  text_lower = \"\".join([c.lower() for c in txt])\n",
        "  txt_nonpunct = \"\".join([c for c in text_lower if c not in string.punctuation])\n",
        "  return txt_nonpunct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKderdL8fzv9"
      },
      "source": [
        "#We call the function \"remove_punctuation\" to be applied on every entry of the \"REVIEW_TEXT\" of our dataframe\n",
        "df['REVIEW_TEXT'] = df['REVIEW_TEXT'].apply(lambda x: remove_punctuation(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLUVirkTh2Ug"
      },
      "source": [
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voXGcrZgh9Kv"
      },
      "source": [
        "Our model consist of 21000 records\n",
        "- Since this is the case we split our data to the format of train, validation and test in the split of 80%, 10%, 10% since we want to have an amount of data to train our model and our data are not offered in a great amount for deep learning purposes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odO8abvAi5DO"
      },
      "source": [
        "# We want to split the data in 80:10:10 for train:valid:test dataset\n",
        "train_size=0.8\n",
        "\n",
        "X = df.copy()\n",
        "\n",
        "# In the first step we will split the data in training and remaining dataset\n",
        "X_train, X_rem= train_test_split(X, train_size=train_size)\n",
        "\n",
        "# # Now since we want the valid and test size to be equal (10% each of overall data). \n",
        "# # we have to define valid_size=0.5 (that is 50% of remaining data)\n",
        "test_size = 0.5\n",
        "X_valid, X_test = train_test_split(X_rem, test_size=0.5)\n",
        "\n",
        "print(\"Training: \", X_train.shape)\n",
        "print(\"Validation: \", X_valid.shape)\n",
        "print(\"Test: \", X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pneAq0CCjwIY"
      },
      "source": [
        "- We further seperate the columns \"RATING\", \"VERIFIED\" from the rest of our dataset since we want to train our model including also those features.  The \"LABEL\" column is our target column which we are going to use to check the accuracy perfomance of our model by comparing the model's predictions with the \"really\" ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIpcA4ssjyKG"
      },
      "source": [
        "# We seperate the \"RATING\" column from the rest of the dataset\n",
        "train_rating = X_train.pop('RATING')\n",
        "valid_rating = X_valid.pop('RATING')\n",
        "test_rating = X_test.pop('RATING')\n",
        "\n",
        "# We seperate the \"VERIFIED_PURCHASE\" column from the rest of the dataset\n",
        "train_pursh = X_train.pop('VERIFIED_PURCHASE')\n",
        "valid_pursh = X_valid.pop('VERIFIED_PURCHASE')\n",
        "test_pursh = X_test.pop('VERIFIED_PURCHASE')\n",
        "\n",
        "# We seperate the \"LABEL\" column from the rest of the dataset\n",
        "train_target = X_train.pop('LABEL')\n",
        "valid_target = X_valid.pop('LABEL')\n",
        "test_target = X_test.pop('LABEL')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjsgklKYmdmW"
      },
      "source": [
        "- Since we have our datasets prepaired in the sense that we have the train, validation and test sets we are now tokenize and pad the \"REVIEW_TEXT\" in order to create our vocabulary. We did not set any vocabulary size for our tokenization process. The padding max size is set to 100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0ozxi8Smr4x"
      },
      "source": [
        "oov_token = \"<OOV>\"\n",
        "max_length = 100\n",
        "padding_type = \"post\"\n",
        "trunction_type=\"post\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjwSQWysmwA9"
      },
      "source": [
        "tokenizer = Tokenizer(oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(X_train.REVIEW_TEXT)\n",
        "vocab_size = len(tokenizer.word_index)+1\n",
        "print('Vocab Size is ',vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLJqSUIhmxNP"
      },
      "source": [
        "#Tokenize and padding for the \"REVIEW_TEXT\" column of the training dataset\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train.REVIEW_TEXT)\n",
        "X_train_padded = pad_sequences(X_train_sequences,maxlen=max_length, padding=padding_type, \n",
        "                       truncating=trunction_type)\n",
        "\n",
        "#Tokenize and padding for the \"REVIEW_TEXT\" column of the validation dataset\n",
        "X_val_sequences = tokenizer.texts_to_sequences(X_valid.REVIEW_TEXT)\n",
        "X_val_padded = pad_sequences(X_val_sequences,maxlen=max_length, \n",
        "                               padding=padding_type, truncating=trunction_type)\n",
        "\n",
        "#Tokenize and padding for the \"REVIEW_TEXT\" column of the test dataset\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test.REVIEW_TEXT)\n",
        "X_test_padded = pad_sequences(X_test_sequences,maxlen=max_length, \n",
        "                               padding=padding_type, truncating=trunction_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sh-MARUCF6Z"
      },
      "source": [
        "# Pre-Trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvrPPEtSCeSS"
      },
      "source": [
        "In order to prepare our model to reach better accuracy we decided to use word-embeddings which is going to help our model to understand the \"meaning\" of each word that is going going to appear during the training procedure. \n",
        "\n",
        "*The main goal of this notebook is to create a model and ckeck it's results under the use of Word2Vec embedding.* \n",
        "\n",
        "For that purpose we used the concept of transfer learning by getting a pre-trained Word2Vec embedding from the gensim library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbdhRyGdnWOw"
      },
      "source": [
        "# We split each text of the \"REVIEW_TEXT\" column\n",
        "documents = []\n",
        "for _text in X_train.REVIEW_TEXT:    \n",
        "    documents.append((_text.split(\" \")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbMN7kmEFBOE"
      },
      "source": [
        "len(documents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xavt_vcdne0G"
      },
      "source": [
        "#Set up the hyperparameters for our Word2Vec model\n",
        "W2V_SIZE = 100\n",
        "W2V_WINDOW = 7\n",
        "W2V_EPOCH = 64\n",
        "W2V_MIN_COUNT = 5\n",
        "\n",
        "#We get the Word2Vec model from gensim\n",
        "w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, \n",
        "                                            window=W2V_WINDOW, \n",
        "                                            min_count=W2V_MIN_COUNT,\n",
        "                                            workers=8)\n",
        "\n",
        "#We build the vocabulary\n",
        "w2v_model.build_vocab(documents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8s2dq4nnivS"
      },
      "source": [
        "#Then we train\n",
        "w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E6JH0z4n4aj"
      },
      "source": [
        "# We test that the word embedding where created successfully by ckecking the most similar words of a given word\n",
        "w2v_model.wv.most_similar(\"bad\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgqbRoZln9r3"
      },
      "source": [
        "#We create our embedding matrix\n",
        "embedding_matrix = np.zeros((vocab_size , W2V_SIZE))\n",
        "for word , i in tokenizer.word_index.items():\n",
        "    if word in w2v_model.wv:\n",
        "        embedding_matrix[i] = w2v_model.wv[word]\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z29wR_GuHR-Y"
      },
      "source": [
        "# The model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM5msX3THnUx"
      },
      "source": [
        "The model that we created is based on the concept that we want to make predictions based on different features of the dataset and to check the resutls based on them. The feautures that seemed to perform better giving a conceptual reason for that are \"REVIEW_TEXT\", \"RATING\", \"VERIFIED_PURCHASE\" given that we want to predict the \"LABEL\" feauture. \n",
        "\n",
        "So our model consists of 3 heads under the following architecture : \n",
        "1. The head that processes the \"**REVIEW_TEXT**\" feature, on which we have applied the Word2Vec word embedding layer as a 100 dimesional vector. Then we use Dropout as a technique to avoid overfitting our model. Then we use a Bi-directional LSTM in order to process the sequences and to be able to extract the \"useful\" information from the individual sentence. Then the output of the Bi-directional LSTM is pass to a fully connected layer which we are going to use later on the concatination step of all our feauture models\n",
        "\n",
        "2. The second head processes the \"**RATING**\" feature using a sequence of dense fully contected which is brought in the same dimensionsion as our first head and thrid layer later on, something that is useful for the concatination process of our heads. \n",
        "\n",
        "3. The third layer follows the same logic as the second one. On this one we process the \"**VERIFIED_PURCHASE**\" feature again using a fully connected dense layer which is brought in the proper dimemsnions for the concatination procedure.\n",
        "\n",
        "Since we have now this three heads in a fully connected dense layer format we concatinate them and we pass them to another fully connected dense layer in order to extract the relations between these features.\n",
        "\n",
        "Then our model is compiled using the ADAM optimizer and as our loss function the binary crossentropy loss. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFSbFslvoO_U"
      },
      "source": [
        "embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n",
        "                            max_length,\n",
        "                            embeddings_initializer= Constant(embedding_matrix),\n",
        "                            trainable=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ffjn9mBhIhy9"
      },
      "source": [
        "review_branch_ip = Input(shape=(100,), name='Review_input')\n",
        "review_branch = embedding_layer(review_branch_ip)\n",
        "review_branch = Dropout(0.2)(review_branch)\n",
        "review_branch = Bidirectional(\n",
        "    LSTM(64, dropout=0.2,recurrent_dropout=0)\n",
        ")(review_branch)\n",
        "review_branch = Dense(64,activation='relu')(review_branch)\n",
        "review_branch_op = Dense(16, activation='relu')(review_branch)\n",
        "\n",
        "\n",
        "rating_branch_ip = Input(shape=(1,), name='Rating_input')\n",
        "rating_branch = Dense(8,activation='relu')(rating_branch_ip)\n",
        "rating_branch = Dropout(0.2)(rating_branch)\n",
        "rating_branch_op = Dense(16,activation='relu')(rating_branch)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "verified_purchase_branch_ip = Input(shape=(1,), name='Verified_Purchase_input')\n",
        "verified_purchase_branch = Dense(8,activation='relu')(verified_purchase_branch_ip)\n",
        "verified_purchase_branch = Dropout(0.2)(verified_purchase_branch)\n",
        "verified_purchase_branch_op = Dense(16,activation='relu')(verified_purchase_branch)\n",
        "\n",
        "\n",
        "concat = concatenate([review_branch_op, rating_branch_op, verified_purchase_branch_op], name='Concatenate')\n",
        "\n",
        "\n",
        "final_op = Dense(8, activation='relu')(concat)\n",
        "final_output = Dense(1,activation='sigmoid')(final_op)\n",
        "\n",
        "model = Model(inputs=[review_branch_ip,rating_branch_ip,verified_purchase_branch_ip], outputs=final_output,\n",
        "                  name='Final_output')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urmgI-lPJs1U"
      },
      "source": [
        "# We compile with Adam and binary crossentropy loss \n",
        "# clipvalue to avoid the gradient exploding\n",
        "model.compile(optimizer=Adam(clipvalue=0.5) , \n",
        "              loss='binary_crossentropy', \n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XazljUb2J7hY"
      },
      "source": [
        "#We display the architecture of our model and display the total parameters that are trainable or not\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B24Xov0tkFFF"
      },
      "source": [
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OLYvNAeKL8O"
      },
      "source": [
        "# This is our first attempt to get the model prediction results having a batch_size of 32 and we train from 10 epochs\n",
        "# We also add the stop early feature in order to avoid training our model while it does not improve\n",
        "stop_early_model = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "history = model.fit([X_train_padded,train_rating,train_pursh], train_target, batch_size=32,epochs=10, validation_data=([X_val_padded,valid_rating,valid_pursh], valid_target),callbacks=[stop_early_model])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhlaz9qKk_Cw"
      },
      "source": [
        "plt.figure(figsize=(16, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OLwlTpNLS11"
      },
      "source": [
        "### Hyperparameters Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kboAZrRPaBuC"
      },
      "source": [
        "Now that we have made a first attempt to get our model's first prediction results in the base of accuracy perfomance, we are going to apply some hyperparameter tuning strategies. For that reason we tested 2 hyperpameter tuning that are included in the \"Keras tuner\" package : \n",
        "\n",
        "1. The first is the **Bayesian Optimization** algorithm which provides a seacrhing strategies based on the logic training the model for the given amount of epochs.\n",
        "2. The we checked also the **Hyperband Optimization** algorithm where we set the maximum amount of epochs and then it applies the hyperparameter tuning in a the range of the given maximun epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP36YlXLLL4m"
      },
      "source": [
        "#We define a function which constinst of the model strucure and our hyperparameter search domain for our model\n",
        "def model_builder(hp):\n",
        "\n",
        "  hp_dropout = hp.Float('Dropout', min_value=0.1, max_value=0.5, step=0.1)\n",
        "  hp_lstm_units = hp.Int('Lstm_units', min_value=8, max_value=64, step=8)\n",
        "  hp_lstm_dropout = hp.Float('Lstm_Dropout', min_value=0.1, max_value=0.5, step=0.1)\n",
        "  # hp_desns_units = hp.Int('Dense_units', min_value=4, max_value=24, step=2)\n",
        "  # hp_learning_rate = hp.Float('Learning_rate', min_value=0.00001, max_value=0.0001, step=-10)\n",
        "\n",
        "\n",
        "  review_branch_ip = Input(shape=(100,), name='Review_input')\n",
        "  review_branch = embedding_layer(review_branch_ip)\n",
        "  review_branch = Dropout(hp_dropout)(review_branch)\n",
        "  review_branch = Bidirectional(\n",
        "      LSTM(hp_lstm_units, dropout=hp_lstm_dropout,recurrent_dropout=0)\n",
        "  )(review_branch)\n",
        "  review_branch = Dense(hp_lstm_units,activation='relu')(review_branch)\n",
        "  review_branch_op = Dense(16, activation='relu')(review_branch)\n",
        " \n",
        "\n",
        "\n",
        "  rating_branch_ip = Input(shape=(1,), name='Rating_input')\n",
        "  rating_branch = Dense(8,activation='relu')(rating_branch_ip)\n",
        "  rating_branch = Dropout(hp_dropout)(rating_branch)\n",
        "  rating_branch_op = Dense(16,activation='relu')(rating_branch)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  verified_purchase_branch_ip = Input(shape=(1,), name='Verified_Purchase_input')\n",
        "  verified_purchase_branch = Dense(8,activation='relu')(verified_purchase_branch_ip)\n",
        "  verified_purchase_branch = Dropout(hp_dropout)(verified_purchase_branch)\n",
        "  verified_purchase_branch_op = Dense(16,activation='relu')(verified_purchase_branch)\n",
        "\n",
        "\n",
        "  concat = concatenate([review_branch_op, rating_branch_op, verified_purchase_branch_op], name='Concatenate')\n",
        "\n",
        "\n",
        "  final_op = Dense(8, activation='relu')(concat)\n",
        "  final_output = Dense(1,activation='sigmoid')(final_op)\n",
        "\n",
        "  model = tf.keras.Model(inputs=[review_branch_ip,rating_branch_ip,verified_purchase_branch_ip], outputs=final_output,\n",
        "                      name='Final_output')\n",
        "\n",
        "  model.compile(optimizer=Adam(clipvalue=0.5) , \n",
        "               loss='binary_crossentropy', \n",
        "               metrics=['acc'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuPXzA2agS3T"
      },
      "source": [
        "#Early stopping\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sMb_f_qggfE"
      },
      "source": [
        "<h5> Bayessian Hyperparameter Tunning </h5>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKEcs3Q0NNXP"
      },
      "source": [
        "# This class sets up our Bayesian Tuner and then possibility to check our model based on different batch sizes\n",
        "class MyTuner_Bayesian(kt.tuners.BayesianOptimization):\n",
        "  def run_trial(self, trial, *args, **kwargs):\n",
        "    \n",
        "    kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', min_value=8, max_value=64, step=8) \n",
        "    \n",
        "    super(MyTuner_Bayesian, self).run_trial(trial, *args, **kwargs)\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkoMX6cxNyal"
      },
      "source": [
        "b_tuner = MyTuner_Bayesian(model_builder,\n",
        "                objective='val_acc',\n",
        "                max_trials = 30,\n",
        "                directory='Bayesian_Tuner',\n",
        "                project_name='Bayesian_Amazon_reviews_tuner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPnfQoLfQF9K"
      },
      "source": [
        "b_tuner.search([X_train_padded,train_rating,train_pursh], train_target, validation_data=([X_val_padded,valid_rating,valid_pursh], valid_target) , callbacks=[stop_early], epochs = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUwU5VFFg36F"
      },
      "source": [
        "# Get the optimal hyperparameters\n",
        "best_b_hps=b_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "#Display the best hyperparameter that perfomed better based on the Bayesian Tuner\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. \n",
        "Dropout: {best_b_hps.get('Dropout')} ,\n",
        "Epochs: {best_b_hps.get('epochs')},\n",
        "Batch_size: {best_b_hps.get('batch_size')},\n",
        "Lstm_units: {best_b_hps.get('Lstm_units')} ,\n",
        "Lstm_Dropout: {best_b_hps.get('Lstm_Dropout')}  \n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsEzS2nigwN5"
      },
      "source": [
        "<h5> Hyperband Hyperparameter Tunning </h5>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yng01-9uxRod"
      },
      "source": [
        "class MyTuner_Hyperband(kt.tuners.Hyperband):\n",
        "  def run_trial(self, trial, *args, **kwargs):\n",
        "    # You can add additional HyperParameters for preprocessing and custom training loops\n",
        "    # via overriding `run_trial`\n",
        "    kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', min_value=8, max_value=64, step=8) #USE 16 \n",
        "    # kwargs['epochs'] = trial.hyperparameters.Int('epochs', min_value=5, max_value=15, step=5) #Int('epochs', 10, 20)\n",
        "    super(MyTuner_Hyperband, self).run_trial(trial, *args, **kwargs) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2rq059iwfZx"
      },
      "source": [
        "h_tuner = MyTuner_Hyperband(model_builder,\n",
        "                     objective='val_acc',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='Hyperband_Tuner',\n",
        "                     project_name='Hyperband_Amazon_reviews_tuner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfKtUFYXxc-n"
      },
      "source": [
        "h_tuner.search([X_train_padded,train_rating,train_pursh], train_target, validation_data=([X_val_padded,valid_rating,valid_pursh], valid_target) , callbacks=[stop_early], epochs = 10)\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGRmpNx5ho7m"
      },
      "source": [
        "# Get the optimal hyperparameters\n",
        "best_h_hps=b_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "\n",
        "#Display the best hyperparameter that perfomed better based on the Bayesian Tuner\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. \n",
        "Dropout: {best_h_hps.get('Dropout')} ,\n",
        "Epochs: {best_h_hps.get('epochs')},\n",
        "Batch_size: {best_h_hps.get('batch_size')},\n",
        "Lstm_units: {best_h_hps.get('Lstm_units')} ,\n",
        "Lstm_Dropout: {best_h_hps.get('Lstm_Dropout')}  \n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US47rJWhllLt"
      },
      "source": [
        "<h5>Final Model</h5>\n",
        "We apply the best hyperparamets found from the two tuning processes and we evaluate the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpqZhjK3mq8K"
      },
      "source": [
        "dropout = best_h_hps.get('Dropout')\n",
        "lstm_units = best_h_hps.get('Lstm_units')\n",
        "lstm_dropout = best_h_hps.get('Lstm_Dropout')\n",
        "epochs = best_h_hps.get('epochs')\n",
        "batch_size  = best_h_hps.get('batch_size')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRRab9SIl8lh"
      },
      "source": [
        "review_branch_ip = Input(shape=(100,), name='Review_input')\n",
        "review_branch = embedding_layer(review_branch_ip)\n",
        "review_branch = Dropout(best_h_hps.get('Dropout'))(review_branch)\n",
        "review_branch = Bidirectional(\n",
        "    LSTM(64, dropout=0.2,recurrent_dropout=0)\n",
        ")(review_branch)\n",
        "review_branch = Dense(64,activation='relu')(review_branch)\n",
        "review_branch_op = Dense(16, activation='relu')(review_branch)\n",
        "\n",
        "\n",
        "rating_branch_ip = Input(shape=(1,), name='Rating_input')\n",
        "rating_branch = Dense(8,activation='relu')(rating_branch_ip)\n",
        "rating_branch = Dropout(0.2)(rating_branch)\n",
        "rating_branch_op = Dense(16,activation='relu')(rating_branch)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "verified_purchase_branch_ip = Input(shape=(1,), name='Verified_Purchase_input')\n",
        "verified_purchase_branch = Dense(8,activation='relu')(verified_purchase_branch_ip)\n",
        "verified_purchase_branch = Dropout(0.2)(verified_purchase_branch)\n",
        "verified_purchase_branch_op = Dense(16,activation='relu')(verified_purchase_branch)\n",
        "\n",
        "\n",
        "concat = concatenate([review_branch_op, rating_branch_op, verified_purchase_branch_op], name='Concatenate')\n",
        "\n",
        "\n",
        "final_op = Dense(8, activation='relu')(concat)\n",
        "final_output = Dense(1,activation='sigmoid')(final_op)\n",
        "\n",
        "model = Model(inputs=[review_branch_ip,rating_branch_ip,verified_purchase_branch_ip], outputs=final_output,\n",
        "                  name='Final_output')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RM4AM6M0kpWn"
      },
      "source": [
        "plt.figure(figsize=(16, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'acc')\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}