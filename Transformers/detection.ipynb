{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import tensorflow as tf\r\n",
    "from transformers import AutoTokenizer\r\n",
    "from transformers import TFAutoModelForSequenceClassification"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "data = pd.read_csv('../Anomaly-Detection/amazon_reviews.txt', delimiter=\"\\t\")\r\n",
    "features = ['RATING', 'REVIEW_TEXT', 'VERIFIED_PURCHASE', 'LABEL']\r\n",
    "\r\n",
    "\r\n",
    "#data_true = data[  data['LABEL'] == '__label1__' ]\r\n",
    "\r\n",
    "data_shortened = data[features]\r\n",
    "\r\n",
    "data_shortened['VERIFIED_PURCHASE'] = data_shortened['VERIFIED_PURCHASE'].replace('N', 0)\r\n",
    "data_shortened['VERIFIED_PURCHASE'] = data_shortened['VERIFIED_PURCHASE'].replace('Y', 1)\r\n",
    "\r\n",
    "data_shortened['LABEL'] = data_shortened['LABEL'].replace('__label1__', 1)\r\n",
    "data_shortened['LABEL'] = data_shortened['LABEL'].replace('__label2__', 0)\r\n",
    "\r\n",
    "#data_pos = data_shortened[  data_shortened['RATING'] == 5 ]\r\n",
    "#data_neg = data_shortened[data_shortened['RATING'] == 1]\r\n",
    "\r\n",
    "#print(data_pos.head())\r\n",
    "\r\n",
    "labels = data_shortened.pop('LABEL')\r\n",
    "\r\n",
    "#print(data_pos.head())\r\n",
    "\r\n",
    "#print(data_pos.dtypes)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-2-5d0a6fe65350>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_shortened['VERIFIED_PURCHASE'] = data_shortened['VERIFIED_PURCHASE'].replace('N', 0)\n",
      "<ipython-input-2-5d0a6fe65350>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_shortened['VERIFIED_PURCHASE'] = data_shortened['VERIFIED_PURCHASE'].replace('Y', 1)\n",
      "<ipython-input-2-5d0a6fe65350>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_shortened['LABEL'] = data_shortened['LABEL'].replace('__label1__', 1)\n",
      "<ipython-input-2-5d0a6fe65350>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_shortened['LABEL'] = data_shortened['LABEL'].replace('__label2__', 0)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "reviews_array = data_shortened[\"REVIEW_TEXT\"].to_numpy(dtype=object)\r\n",
    "labels_array = labels.to_numpy(dtype=np.int32)\r\n",
    "\r\n",
    "#Shuffling the dataset\r\n",
    "data_length = len(reviews_array)\r\n",
    "idx = np.random.permutation(data_length)\r\n",
    "shuffled_reviews_array = reviews_array[idx]\r\n",
    "shuffled_labels_array = labels_array[idx]\r\n",
    "\r\n",
    "# Train-test split\r\n",
    "train_ratio = 0.8\r\n",
    "train_data_len = int( train_ratio * data_length )\r\n",
    "\r\n",
    "train_reviews_array = shuffled_reviews_array[:train_data_len]\r\n",
    "train_labels_array = shuffled_labels_array[:train_data_len]\r\n",
    "\r\n",
    "test_reviews_array = shuffled_reviews_array[train_data_len:]\r\n",
    "test_labels_array = shuffled_labels_array[train_data_len:]\r\n",
    "\r\n",
    "model_name = 'distilbert-base-uncased' # or bert-base-cased\r\n",
    "\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n",
    "#tokenized_df = tokenizer(data_shortened.head()[\"REVIEW_TEXT\"].tolist(), padding=\"max_length\", truncation=True)\r\n",
    "\r\n",
    "train_reviews = train_reviews_array.tolist()\r\n",
    "test_reviews = test_reviews_array.tolist()\r\n",
    "\r\n",
    "#! Take Care\r\n",
    "training_size = 200\r\n",
    "\r\n",
    "train_reviews_small = train_reviews[0:training_size]\r\n",
    "train_labels_array_small = train_labels_array[0:training_size]\r\n",
    "test_reviews_small = test_reviews[0:training_size]\r\n",
    "test_labels_array_small = test_labels_array[0:training_size]\r\n",
    "\r\n",
    "\r\n",
    "train_tokenized_df = tokenizer(train_reviews_small, padding=\"max_length\", truncation=True)\r\n",
    "test_tokenized_df = tokenizer(test_reviews_small, padding=\"max_length\", truncation=True)\r\n",
    "\r\n",
    "#print( np.sum(train_labels_array == 0))\r\n",
    "#print(train_labels_array[0:5])\r\n",
    "\r\n",
    "#print(type(tokenized_df))\r\n",
    "#print(tokenized_df.keys())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 11.2kB/s]\n",
      "Downloading: 100%|██████████| 442/442 [00:00<00:00, 221kB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 1.43MB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 1.50MB/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\r\n",
    "#Create the input dictionary for the model\r\n",
    "train_features = {x: tf.convert_to_tensor(train_tokenized_df[x]) for x in tokenizer.model_input_names}\r\n",
    "test_features = {x: tf.convert_to_tensor(test_tokenized_df[x]) for x in tokenizer.model_input_names} \r\n",
    "#print(train_features)\r\n",
    "\r\n",
    "#The dataset now is a tuple of dictionary containint inpud_ids, token_type_ids. attention masks        and  a tf tensor containing the value for the label\r\n",
    "\r\n",
    "#train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features, labels.head()))\r\n",
    "train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels_array_small))\r\n",
    "train_tf_dataset = train_tf_dataset.shuffle(training_size).batch(8) #train_data_len\r\n",
    "\r\n",
    "print(train_tf_dataset)\r\n",
    "\r\n",
    "test_tf_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_labels_array_small))\r\n",
    "test_tf_dataset = test_tf_dataset.batch(8)\r\n",
    "\r\n",
    "print(test_labels_array_small.dtype)\r\n",
    "print(train_labels_array_small.dtype)\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<BatchDataset shapes: ({input_ids: (None, 512), attention_mask: (None, 512)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>\n",
      "int32\n",
      "int32\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import tensorflow.keras as keras\r\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\r\n",
    "callbacks = [\r\n",
    "    keras.callbacks.EarlyStopping(monitor=\"accuracy\", patience=15,\r\n",
    "                                    verbose=1, mode=\"min\", restore_best_weights=True),\r\n",
    "        keras.callbacks.ModelCheckpoint(filepath=\"transformer_model/model_distillbert_200.hdf5\", verbose=1, save_best_only=True)\r\n",
    "    ]\r\n",
    "\r\n",
    "model.compile(\r\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\r\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy(),\r\n",
    "    \r\n",
    ")\r\n",
    "\r\n",
    "model.fit(train_tf_dataset, validation_data=test_tf_dataset, epochs=10, verbose=1, callbacks = callbacks) #3 epochs\r\n",
    "\r\n",
    "model.save_pretrained(\"transformer_model\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 363M/363M [00:10<00:00, 35.4MB/s]\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'activation_13', 'vocab_layer_norm', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFDistilBertForSequenceClassification.call of <transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification object at 0x000001902C88B580>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFDistilBertForSequenceClassification.call of <transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification object at 0x000001902C88B580>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFDistilBertMainLayer.call of <transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertMainLayer object at 0x000001902CB66A30>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFDistilBertMainLayer.call of <transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertMainLayer object at 0x000001902CB66A30>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFEmbeddings.call of <transformers.models.distilbert.modeling_tf_distilbert.TFEmbeddings object at 0x000001902CB66D90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFEmbeddings.call of <transformers.models.distilbert.modeling_tf_distilbert.TFEmbeddings object at 0x000001902CB66D90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFTransformer.call of <transformers.models.distilbert.modeling_tf_distilbert.TFTransformer object at 0x000001902CD985B0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFTransformer.call of <transformers.models.distilbert.modeling_tf_distilbert.TFTransformer object at 0x000001902CD985B0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFTransformerBlock.call of <transformers.models.distilbert.modeling_tf_distilbert.TFTransformerBlock object at 0x000001902CD98820>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFTransformerBlock.call of <transformers.models.distilbert.modeling_tf_distilbert.TFTransformerBlock object at 0x000001902CD98820>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFMultiHeadSelfAttention.call of <transformers.models.distilbert.modeling_tf_distilbert.TFMultiHeadSelfAttention object at 0x000001902CD98CA0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFMultiHeadSelfAttention.call of <transformers.models.distilbert.modeling_tf_distilbert.TFMultiHeadSelfAttention object at 0x000001902CD98CA0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6995 - sparse_categorical_accuracy: 0.5200WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "25/25 [==============================] - 283s 11s/step - loss: 0.6995 - sparse_categorical_accuracy: 0.5200 - val_loss: 0.6914 - val_sparse_categorical_accuracy: 0.5150\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 265s 11s/step - loss: 0.6913 - sparse_categorical_accuracy: 0.5200 - val_loss: 0.6809 - val_sparse_categorical_accuracy: 0.5850\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 264s 11s/step - loss: 0.5787 - sparse_categorical_accuracy: 0.7250 - val_loss: 0.6853 - val_sparse_categorical_accuracy: 0.5850\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 257s 10s/step - loss: 0.2627 - sparse_categorical_accuracy: 0.9200 - val_loss: 1.1081 - val_sparse_categorical_accuracy: 0.5350\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 261s 10s/step - loss: 0.0913 - sparse_categorical_accuracy: 0.9700 - val_loss: 1.0731 - val_sparse_categorical_accuracy: 0.5800\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 261s 10s/step - loss: 0.0708 - sparse_categorical_accuracy: 0.9800 - val_loss: 1.5387 - val_sparse_categorical_accuracy: 0.5450\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 266s 11s/step - loss: 0.0237 - sparse_categorical_accuracy: 0.9900 - val_loss: 1.2815 - val_sparse_categorical_accuracy: 0.6350\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 261s 10s/step - loss: 0.0312 - sparse_categorical_accuracy: 0.9850 - val_loss: 1.5936 - val_sparse_categorical_accuracy: 0.5700\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 258s 10s/step - loss: 0.0144 - sparse_categorical_accuracy: 0.9950 - val_loss: 1.8295 - val_sparse_categorical_accuracy: 0.5550\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 258s 10s/step - loss: 0.0055 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.6540 - val_sparse_categorical_accuracy: 0.5800\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "\r\n",
    "#print(data_pos.head()[\"REVIEW_TEXT\"].tolist()[0])\r\n",
    "\r\n",
    "print(train_labels_array[200:205])\r\n",
    "\r\n",
    "for i in range(200, 205):\r\n",
    "    #y = tokenizer(data_shortened.head()[\"REVIEW_TEXT\"].tolist()[i], padding=\"max_length\", truncation=True)\r\n",
    "    y = tokenizer(train_reviews[i], padding=\"max_length\", truncation=True)\r\n",
    "    y['input_ids'] = tf.convert_to_tensor( [y['input_ids']] )\r\n",
    "    if model_name == 'bert-base-cased':\r\n",
    "        y['token_type_ids'] = tf.convert_to_tensor( [y['token_type_ids']] )\r\n",
    "    y['attention_mask'] = tf.convert_to_tensor( [y['attention_mask']] )\r\n",
    "\r\n",
    "    label = model(y)\r\n",
    "    probs = tf.nn.softmax(label.logits)\r\n",
    "    print(probs)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1 1 0 1 0]\n",
      "tf.Tensor([[0.99846727 0.00153272]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[0.35731387 0.6426861 ]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[0.00899104 0.99100894]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[0.00801229 0.9919877 ]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[0.00160082 0.9983992 ]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('nlp': conda)"
  },
  "interpreter": {
   "hash": "f10a50ba453e9665cd1395d2fc3efc46444869c32c9e55f4e0a94563901bc1da"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}