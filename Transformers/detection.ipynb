{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import tensorflow as tf\r\n",
    "from transformers import AutoTokenizer\r\n",
    "from transformers import TFAutoModelForSequenceClassification"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "data = pd.read_csv('../Anomaly-Detection/amazon_reviews.txt', delimiter=\"\\t\")\r\n",
    "features = ['RATING', 'REVIEW_TEXT', 'VERIFIED_PURCHASE', 'LABEL']\r\n",
    "data_true = data[  data['LABEL'] == '__label1__' ]\r\n",
    "data_shortened = data_true[features]\r\n",
    "\r\n",
    "data_shortened['VERIFIED_PURCHASE'] = data_shortened['VERIFIED_PURCHASE'].replace('N', 0)\r\n",
    "data_shortened['VERIFIED_PURCHASE'] = data_shortened['VERIFIED_PURCHASE'].replace('Y', 1)\r\n",
    "\r\n",
    "data_shortened['LABEL'] = data_shortened['LABEL'].replace('__label1__', 1)\r\n",
    "data_shortened['LABEL'] = data_shortened['LABEL'].replace('__label2__', 0)\r\n",
    "\r\n",
    "#data_pos = data_shortened[  data_shortened['RATING'] == 5 ]\r\n",
    "#data_neg = data_shortened[data_shortened['RATING'] == 1]\r\n",
    "\r\n",
    "#print(data_pos.head())\r\n",
    "\r\n",
    "labels = data_shortened.pop('LABEL')\r\n",
    "\r\n",
    "#print(data_pos.head())\r\n",
    "\r\n",
    "#print(data_pos.dtypes)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-2-085cdb72aa0e>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_shortened['VERIFIED_PURCHASE'] = data_shortened['VERIFIED_PURCHASE'].replace('N', 0)\n",
      "<ipython-input-2-085cdb72aa0e>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_shortened['VERIFIED_PURCHASE'] = data_shortened['VERIFIED_PURCHASE'].replace('Y', 1)\n",
      "<ipython-input-2-085cdb72aa0e>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_shortened['LABEL'] = data_shortened['LABEL'].replace('__label1__', 1)\n",
      "<ipython-input-2-085cdb72aa0e>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_shortened['LABEL'] = data_shortened['LABEL'].replace('__label2__', 0)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\r\n",
    "\r\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\r\n",
    "tokenized_df = tokenizer(data_shortened.head()[\"REVIEW_TEXT\"].tolist(), padding=\"max_length\", truncation=True)\r\n",
    "\r\n",
    "print(type(tokenized_df))\r\n",
    "print(tokenized_df.keys())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\r\n",
    "train_features = {x: tf.convert_to_tensor(tokenized_df[x]) for x in tokenizer.model_input_names}\r\n",
    "#print(train_features)\r\n",
    "\r\n",
    "#The dataset now is a tuple of dictionary containint inpud_ids, token_type_ids. attention masks        and  a tf tensor containing the value for the label\r\n",
    "train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features, labels.head()))\r\n",
    "\r\n",
    "#print(train_tf_dataset)\r\n",
    "train_tf_dataset = train_tf_dataset.shuffle(5).batch(5)\r\n",
    "\r\n",
    "print(train_tf_dataset)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<BatchDataset shapes: ({input_ids: (None, 512), token_type_ids: (None, 512), attention_mask: (None, 512)}, (None,)), types: ({input_ids: tf.int32, token_type_ids: tf.int32, attention_mask: tf.int32}, tf.int64)>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained('bert-base-cased', num_labels=2)\r\n",
    "\r\n",
    "\r\n",
    "model.compile(\r\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\r\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy(),\r\n",
    ")\r\n",
    "\r\n",
    "model.fit(train_tf_dataset, validation_data=train_tf_dataset, epochs=3, verbose=1)\r\n",
    "\r\n",
    "model.save_pretrained(\"transformer_model\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertForSequenceClassification.call of <transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x00000200C3CCABE0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertForSequenceClassification.call of <transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x00000200C3CCABE0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertMainLayer.call of <transformers.models.bert.modeling_tf_bert.TFBertMainLayer object at 0x00000200C36AD3D0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertMainLayer.call of <transformers.models.bert.modeling_tf_bert.TFBertMainLayer object at 0x00000200C36AD3D0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertEmbeddings.call of <transformers.models.bert.modeling_tf_bert.TFBertEmbeddings object at 0x00000200C36AD610>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertEmbeddings.call of <transformers.models.bert.modeling_tf_bert.TFBertEmbeddings object at 0x00000200C36AD610>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertEncoder.call of <transformers.models.bert.modeling_tf_bert.TFBertEncoder object at 0x00000200C36A9370>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertEncoder.call of <transformers.models.bert.modeling_tf_bert.TFBertEncoder object at 0x00000200C36A9370>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertLayer.call of <transformers.models.bert.modeling_tf_bert.TFBertLayer object at 0x00000200C369FBB0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertLayer.call of <transformers.models.bert.modeling_tf_bert.TFBertLayer object at 0x00000200C369FBB0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertAttention.call of <transformers.models.bert.modeling_tf_bert.TFBertAttention object at 0x00000200C369FEE0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertAttention.call of <transformers.models.bert.modeling_tf_bert.TFBertAttention object at 0x00000200C369FEE0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertSelfAttention.call of <transformers.models.bert.modeling_tf_bert.TFBertSelfAttention object at 0x00000200C369F460>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertSelfAttention.call of <transformers.models.bert.modeling_tf_bert.TFBertSelfAttention object at 0x00000200C369F460>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFBertPooler.call of <transformers.models.bert.modeling_tf_bert.TFBertPooler object at 0x00000200C36A9130>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertPooler.call of <transformers.models.bert.modeling_tf_bert.TFBertPooler object at 0x00000200C36A9130>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9506 - sparse_categorical_accuracy: 0.0000e+00WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.9506 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 0.6957 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.7097 - sparse_categorical_accuracy: 0.8000 - val_loss: 0.3291 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.3291 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2763 - val_sparse_categorical_accuracy: 1.0000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "\r\n",
    "#print(data_pos.head()[\"REVIEW_TEXT\"].tolist()[0])\r\n",
    "\r\n",
    "print(labels.head())\r\n",
    "\r\n",
    "for i in range(5):\r\n",
    "    y = tokenizer(data_shortened.head()[\"REVIEW_TEXT\"].tolist()[i], padding=\"max_length\", truncation=True)\r\n",
    "\r\n",
    "    y['input_ids'] = tf.convert_to_tensor( [y['input_ids']] )\r\n",
    "    y['token_type_ids'] = tf.convert_to_tensor( [y['token_type_ids']] )\r\n",
    "    y['attention_mask'] = tf.convert_to_tensor( [y['attention_mask']] )\r\n",
    "\r\n",
    "    label = model(y)\r\n",
    "    probs = tf.nn.softmax(label.logits)\r\n",
    "    print(probs)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: LABEL, dtype: int64\n",
      "tf.Tensor([[0.2620531  0.73794687]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[0.22877496 0.77122504]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[0.23225027 0.76774967]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[0.2602506 0.7397495]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[0.22276734 0.77723265]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('nlp': conda)"
  },
  "interpreter": {
   "hash": "f10a50ba453e9665cd1395d2fc3efc46444869c32c9e55f4e0a94563901bc1da"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}